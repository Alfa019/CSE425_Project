{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory: Multimodal Music Clustering\n",
    "Spotify tabular features + Bengali lyrics + audio MFCC, with PCA/KMeans, (Beta-)VAE, and hybrid fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ensure src is importable\n",
    "repo_root = os.path.abspath('..')\n",
    "sys.path.append(os.path.join(repo_root, 'src'))\n",
    "\n",
    "from dataset import (\n",
    "    load_spotify_features, load_bengali_lyrics, lyrics_tfidf_svd,\n",
    "    list_wav_files, extract_mfcc_features, fuse_audio_text_embeddings,\n",
    "    load_audio_paths_and_labels, prefilter_readable_files,\n",
    "    RobustMelSpecDataset, drop_none_collate,\n",
    "    load_bangla_lyrics_texts, load_spotify_texts, make_text_embeddings,\n",
    "    fuse_hybrid_features,\n",
    "    DEFAULT_SPOTIFY_CSV, DEFAULT_LYRICS_BN_ROOT, DEFAULT_AUDIO_ROOT,\n",
    ")\n",
    "from clustering import tune_kmeans, cluster_kmeans, pca_reduce, plot_tsne, run_clusterings\n",
    "from evaluation import eval_clustering\n",
    "from vae import (\n",
    "    train_vae, encode_mu,\n",
    "    ConvVAE, ConvAE, train_convvae, train_ae, extract_latents,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "OUT_DIR = os.path.abspath(os.path.join(repo_root, 'results'))\n",
    "VIS_DIR = os.path.join(OUT_DIR, 'latent_visualization')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "\n",
    "def save_fig(name):\n",
    "    path = os.path.join(VIS_DIR, name)\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    print('Saved:', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Spotify tabular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOTIFY_CSV = DEFAULT_SPOTIFY_CSV\n",
    "FEATURES_SPOTIFY = [\n",
    "    'danceability','energy','valence','acousticness',\n",
    "    'instrumentalness','speechiness','liveness','tempo','loudness'\n",
    "]\n",
    "\n",
    "dfA, Xz, scalerA = load_spotify_features(SPOTIFY_CSV, FEATURES_SPOTIFY, n_sample=60000, seed=SEED)\n",
    "missing = [c for c in FEATURES_SPOTIFY if c not in dfA.columns]\n",
    "if missing:\n",
    "    raise ValueError(f'Missing expected Spotify columns: {missing}')\n",
    "dfA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotify EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfA[FEATURES_SPOTIFY].describe().T)\n",
    "\n",
    "if 'popularity' in dfA.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(dfA['popularity'], bins=50, kde=True)\n",
    "    plt.title('Popularity Distribution (Spotify)')\n",
    "    save_fig('spotify_popularity.png')\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "corr = pd.DataFrame(Xz, columns=FEATURES_SPOTIFY).corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap (Standardized Spotify Features)')\n",
    "save_fig('spotify_corr_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp, pca = pca_reduce(Xz, n_components=8, seed=SEED)\n",
    "baseline_pca = tune_kmeans(Xp, list(range(4, 13)), seed=SEED, n_init=30)\n",
    "baseline_pca.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK_pca = int(baseline_pca.iloc[0]['K'])\n",
    "labels_pca, km_pca = cluster_kmeans(Xp, bestK_pca, seed=SEED, n_init=30)\n",
    "plot_tsne(Xp, labels_pca, f't-SNE: PCA features (K={bestK_pca})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A2: (Beta-)VAE on Spotify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, best_state, hist = train_vae(Xz, hidden_dim=256, latent_dim=10, epochs=35, lr=3e-3, seed=SEED, beta_max=4.0, warmup=10)\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(data=hist, x='epoch', y='train_loss', label='train')\n",
    "sns.lineplot(data=hist, x='epoch', y='val_loss', label='val')\n",
    "sns.lineplot(data=hist, x='epoch', y='train_recon', label='train_recon')\n",
    "sns.lineplot(data=hist, x='epoch', y='val_recon', label='val_recon')\n",
    "sns.lineplot(data=hist, x='epoch', y='train_kld', label='train_kld')\n",
    "sns.lineplot(data=hist, x='epoch', y='val_kld', label='val_kld')\n",
    "plt.title('VAE Loss Curves')\n",
    "save_fig('spotify_vae_loss_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(best_state)\n",
    "Z = encode_mu(vae, Xz)\n",
    "vae_k = tune_kmeans(Z, list(range(4, 13)), seed=SEED, n_init=50)\n",
    "vae_k.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK_vae = int(vae_k.iloc[0]['K'])\n",
    "labels_vae, km_vae = cluster_kmeans(Z, bestK_vae, seed=SEED, n_init=50)\n",
    "plot_tsne(Z, labels_vae, f't-SNE: VAE latent (K={bestK_vae})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_baseline = baseline_pca.iloc[0].to_dict()\n",
    "best_vae = vae_k.iloc[0].to_dict()\n",
    "compare = pd.DataFrame([\n",
    "    {'method': 'PCA+KMeans', **best_baseline},\n",
    "    {'method': '(Beta-)VAE+KMeans', **best_vae},\n",
    "])[['method','K','silhouette','calinski_harabasz','davies_bouldin']]\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA_out = dfA.reset_index(drop=True).copy()\n",
    "dfA_out['cluster_pca'] = labels_pca\n",
    "dfA_out['cluster_vae'] = labels_vae\n",
    "profile = dfA_out.groupby('cluster_vae')[FEATURES_SPOTIFY].mean()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(profile, cmap='viridis')\n",
    "plt.title('VAE Cluster Profiles (Mean Feature Values)')\n",
    "save_fig('spotify_vae_cluster_profiles.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Bengali lyrics (TF-IDF + SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS_BN_ROOT = DEFAULT_LYRICS_BN_ROOT\n",
    "bn, text_col, bn_csv = load_bengali_lyrics(LYRICS_BN_ROOT)\n",
    "bn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = bn.dropna(subset=[text_col]).copy()\n",
    "bn['lyrics_text'] = bn[text_col].astype(str)\n",
    "bn['n_chars'] = bn['lyrics_text'].str.len()\n",
    "bn['n_words'] = bn['lyrics_text'].str.split().apply(len)\n",
    "display(bn[['n_chars','n_words']].describe().T)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.histplot(bn['n_words'], bins=60, kde=True)\n",
    "plt.title('Lyrics Length Distribution (words)')\n",
    "save_fig('lyrics_length_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_small, Z_text_z, tfidf, svd, scalerB = lyrics_tfidf_svd(bn, text_col='lyrics_text', max_lyrics=20000, svd_dim=128, seed=SEED)\n",
    "lyrics_k = tune_kmeans(Z_text_z, list(range(4, 13)), seed=SEED, n_init=30)\n",
    "lyrics_k.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained = svd.explained_variance_ratio_.sum()\n",
    "print(f'SVD explained variance ratio (sum over {svd.n_components} dims): {explained:.4f}')\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.lineplot(x=np.arange(1, svd.n_components + 1), y=np.cumsum(svd.explained_variance_ratio_))\n",
    "plt.title('Cumulative Explained Variance (Lyrics SVD)')\n",
    "plt.xlabel('SVD Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "save_fig('lyrics_svd_explained_variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK_lyrics = int(lyrics_k.iloc[0]['K'])\n",
    "labels_lyrics, km_lyrics = cluster_kmeans(Z_text_z, bestK_lyrics, seed=SEED, n_init=30)\n",
    "plot_tsne(Z_text_z, labels_lyrics, f't-SNE: Bengali lyrics TF-IDF+SVD (K={bestK_lyrics})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Audio MFCC (wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_ROOT = DEFAULT_AUDIO_ROOT\n",
    "wav_files = list_wav_files(AUDIO_ROOT)\n",
    "print('Found wav files:', len(wav_files))\n",
    "\n",
    "if len(wav_files) > 0:\n",
    "    sizes = np.array([os.path.getsize(p) for p in wav_files])\n",
    "    print(pd.Series(sizes).describe())\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(sizes, bins=50, kde=True)\n",
    "    plt.title('Audio File Size Distribution (bytes)')\n",
    "    save_fig('audio_file_size_distribution.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa_z, bad_files = extract_mfcc_features(wav_files, max_audio=3000, sr=22050, n_mfcc=20)\n",
    "print('Bad/corrupted skipped:', len(bad_files))\n",
    "\n",
    "Xa_p, pca_a = pca_reduce(Xa_z, n_components=16, seed=SEED)\n",
    "audio_baseline = tune_kmeans(Xa_p, list(range(4, 13)), seed=SEED, n_init=30)\n",
    "audio_baseline.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK_audio = int(audio_baseline.iloc[0]['K'])\n",
    "labels_audio, km_audio = cluster_kmeans(Xa_p, bestK_audio, seed=SEED, n_init=30)\n",
    "plot_tsne(Xa_p, labels_audio, f't-SNE: Audio MFCC PCA (K={bestK_audio})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Multimodal fusion (unpaired baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fz = fuse_audio_text_embeddings(Xa_z, Z_text_z, fuse_dim_audio=64, fuse_dim_text=64, seed=SEED)\n",
    "fuse_k = tune_kmeans(Fz, list(range(4, 13)), seed=SEED, n_init=50)\n",
    "fuse_k.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK_fuse = int(fuse_k.iloc[0]['K'])\n",
    "labels_fuse, km_fuse = cluster_kmeans(Fz, bestK_fuse, seed=SEED, n_init=50)\n",
    "plot_tsne(Fz, labels_fuse, f't-SNE: Fused (Audio+Lyrics) vectors (K={bestK_fuse})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Save metrics and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pca.to_csv(os.path.join(OUT_DIR, 'spotify_baseline_pca_kmeans.csv'), index=False)\n",
    "vae_k.to_csv(os.path.join(OUT_DIR, 'spotify_vae_kmeans.csv'), index=False)\n",
    "compare.to_csv(os.path.join(OUT_DIR, 'spotify_best_comparison.csv'), index=False)\n",
    "lyrics_k.to_csv(os.path.join(OUT_DIR, 'bengali_lyrics_kmeans.csv'), index=False)\n",
    "audio_baseline.to_csv(os.path.join(OUT_DIR, 'audio_mfcc_baseline.csv'), index=False)\n",
    "fuse_k.to_csv(os.path.join(OUT_DIR, 'fusion_kmeans.csv'), index=False)\n",
    "\n",
    "dfA_out.to_csv(os.path.join(OUT_DIR, 'spotify_with_clusters.csv'), index=False)\n",
    "\n",
    "print('Saved outputs to:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F: Medium task (ConvVAE + hybrid clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTZAN_DIR = os.path.abspath(os.path.join(DEFAULT_AUDIO_ROOT, '..'))\n",
    "audio_files, audio_y, label_to_id = load_audio_paths_and_labels(GTZAN_DIR)\n",
    "print('Found files:', len(audio_files), 'genres:', len(label_to_id))\n",
    "\n",
    "max_items = min(1000, len(audio_files))\n",
    "sel = np.random.permutation(len(audio_files))[:max_items]\n",
    "audio_files_sub = [audio_files[i] for i in sel]\n",
    "audio_y_sub = audio_y[sel]\n",
    "\n",
    "good_files, good_labels, rep, bad_df = prefilter_readable_files(audio_files_sub, audio_y_sub, sr=22050, duration=30.0, max_check=None)\n",
    "rep.to_csv(os.path.join(OUT_DIR, 'gtzan_decode_report.csv'), index=False)\n",
    "bad_df.to_csv(os.path.join(OUT_DIR, 'gtzan_bad_files.csv'), index=False)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = RobustMelSpecDataset(good_files, good_labels, sr=22050, seconds=30, n_mels=128, target_frames=128)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True, num_workers=0, collate_fn=drop_none_collate)\n",
    "print('Usable samples:', len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convvae = ConvVAE(latent_dim=32)\n",
    "hist_conv = train_convvae(convvae, dl, epochs=15, beta_start=0.0, beta_end=1.0)\n",
    "hist_conv.to_csv(os.path.join(OUT_DIR, 'medium_convvae_training.csv'), index=False)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(hist_conv['epoch'], hist_conv['loss'], label='loss')\n",
    "plt.plot(hist_conv['epoch'], hist_conv['recon'], label='recon')\n",
    "plt.plot(hist_conv['epoch'], hist_conv['kl'], label='kl')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('ConvVAE training curves')\n",
    "plt.legend()\n",
    "save_fig('medium_convvae_training.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convvae.eval()\n",
    "audio_latents, audio_labels = extract_latents(convvae, DataLoader(ds, batch_size=64, shuffle=False, num_workers=0, collate_fn=drop_none_collate))\n",
    "print('Audio latents:', audio_latents.shape)\n",
    "pd.DataFrame(audio_latents).to_csv(os.path.join(OUT_DIR, 'medium_audio_latents.csv'), index=False)\n",
    "pd.DataFrame({'genre_id': audio_labels}).to_csv(os.path.join(OUT_DIR, 'medium_audio_labels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_texts = load_bangla_lyrics_texts(DEFAULT_LYRICS_BN_ROOT, max_docs=2000)\n",
    "lyrics_emb, tfidf_m, svd_m = make_text_embeddings(lyrics_texts, max_features=20000, ngram_range=(1,2), svd_dim=128, seed=SEED)\n",
    "pd.DataFrame(lyrics_emb).to_csv(os.path.join(OUT_DIR, 'medium_lyrics_embeddings.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_scaled = fuse_hybrid_features(audio_latents, lyrics_emb, seed=SEED)\n",
    "pd.DataFrame(hybrid_scaled).to_csv(os.path.join(OUT_DIR, 'medium_hybrid_features_scaled.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(label_to_id)\n",
    "metrics_df = run_clusterings(hybrid_scaled, audio_labels, K, seed=SEED)\n",
    "metrics_df = metrics_df.sort_values(by=['ari','silhouette'], ascending=False)\n",
    "metrics_df.to_csv(os.path.join(OUT_DIR, 'medium_clustering_metrics.csv'), index=False)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "viz_n = min(1500, hybrid_scaled.shape[0])\n",
    "viz_idx = np.random.permutation(hybrid_scaled.shape[0])[:viz_n]\n",
    "X_viz = hybrid_scaled[viz_idx]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=SEED, init='pca', learning_rate='auto', perplexity=30)\n",
    "Z_tsne = tsne.fit_transform(X_viz)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Z_tsne[:,0], Z_tsne[:,1], c=audio_labels[viz_idx], s=10)\n",
    "plt.title('t-SNE | Hybrid | colored by genre')\n",
    "save_fig('medium_tsne_hybrid_by_genre.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G: Hard task (Beta-VAE + multilingual fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_texts = load_bangla_lyrics_texts(DEFAULT_LYRICS_BN_ROOT, max_docs=2000)\n",
    "en_texts = load_spotify_texts(DEFAULT_SPOTIFY_CSV, max_docs=2000)\n",
    "\n",
    "all_texts = bn_texts + en_texts\n",
    "X_txt, tfidf_h, svd_h = make_text_embeddings(all_texts, max_features=25000, ngram_range=(1,2), svd_dim=128, seed=SEED)\n",
    "bn_emb = X_txt[:len(bn_texts)]\n",
    "en_emb = X_txt[len(bn_texts):]\n",
    "\n",
    "pd.DataFrame(bn_emb).to_csv(os.path.join(OUT_DIR, 'hard_bn_lyrics_emb.csv'), index=False)\n",
    "pd.DataFrame(en_emb).to_csv(os.path.join(OUT_DIR, 'hard_en_proxy_emb.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [1.0, 4.0, 8.0]\n",
    "beta_models = {}\n",
    "beta_hist_all = []\n",
    "\n",
    "for b in betas:\n",
    "    m = ConvVAE(latent_dim=32)\n",
    "    h = train_convvae(m, dl, epochs=12, beta_start=b, beta_end=b)\n",
    "    beta_models[b] = m\n",
    "    h['beta'] = b\n",
    "    beta_hist_all.append(h)\n",
    "\n",
    "hist_df = pd.concat(beta_hist_all, ignore_index=True)\n",
    "hist_df.to_csv(os.path.join(OUT_DIR, 'hard_beta_vae_training.csv'), index=False)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "for b in betas:\n",
    "    sub = hist_df[hist_df['beta'] == b]\n",
    "    plt.plot(sub['epoch'], sub['loss'], label=f'beta={b}')\n",
    "plt.title('Beta-VAE loss curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "save_fig('hard_beta_vae_loss_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = ConvAE(latent_dim=32)\n",
    "ae_hist = train_ae(ae_model, dl, epochs=10)\n",
    "ae_hist.to_csv(os.path.join(OUT_DIR, 'hard_ae_training.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_recon_grid(model, ds, n=4, title='Recon'):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    plt.figure(figsize=(10, 2.5*n))\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            x, y = ds[i]\n",
    "            x_in = torch.tensor(x).unsqueeze(0).to(device)\n",
    "            xhat, _, _ = model(x_in)\n",
    "            x_np = x.squeeze(0)\n",
    "            xhat_np = xhat.squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            plt.subplot(n, 2, 2*i+1)\n",
    "            plt.imshow(x_np, aspect='auto', origin='lower')\n",
    "            plt.title(f'Original (label={y})')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(n, 2, 2*i+2)\n",
    "            plt.imshow(xhat_np, aspect='auto', origin='lower')\n",
    "            plt.title('Reconstruction')\n",
    "            plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    save_fig('hard_beta_vae_recon_grid.png')\n",
    "    plt.show()\n",
    "\n",
    "show_recon_grid(beta_models[betas[0]], ds, n=4, title=f'Beta-VAE Reconstructions (beta={betas[0]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_Z = {}\n",
    "for b, m in beta_models.items():\n",
    "    Zb, Yb = extract_latents(m, DataLoader(ds, batch_size=64, shuffle=False, num_workers=0, collate_fn=drop_none_collate))\n",
    "    beta_Z[b] = (Zb, Yb)\n",
    "\n",
    "Z_ae, Y_ae = extract_latents(ae_model, DataLoader(ds, batch_size=64, shuffle=False, num_workers=0, collate_fn=drop_none_collate))\n",
    "\n",
    "N = len(ds)\n",
    "rng = np.random.default_rng(SEED)\n",
    "lang = rng.integers(0, 2, size=N)\n",
    "bn_idx = rng.integers(0, bn_emb.shape[0], size=N)\n",
    "en_idx = rng.integers(0, en_emb.shape[0], size=N)\n",
    "txt_vecs = np.where(lang[:,None] == 0, bn_emb[bn_idx], en_emb[en_idx]).astype(np.float32)\n",
    "\n",
    "K = len(label_to_id)\n",
    "genre_oh = np.eye(K, dtype=np.float32)[audio_y[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    purity_sum = 0\n",
    "    for c in np.unique(y_pred):\n",
    "        idx = np.where(y_pred == c)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        _, counts = np.unique(y_true[idx], return_counts=True)\n",
    "        purity_sum += counts.max()\n",
    "    return purity_sum / n\n",
    "\n",
    "def run_kmeans_eval(X, y_true, k, name):\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    labels = KMeans(n_clusters=k, random_state=SEED, n_init='auto').fit_predict(Xs)\n",
    "    return {\n",
    "        'method': name,\n",
    "        'silhouette': float(np.nan) if len(np.unique(labels)) < 2 else float(eval_clustering(Xs, labels)['silhouette']),\n",
    "        'nmi': float(normalized_mutual_info_score(y_true, labels)),\n",
    "        'ari': float(adjusted_rand_score(y_true, labels)),\n",
    "        'purity': float(purity_score(y_true, labels)),\n",
    "        'n_clusters': int(len(np.unique(labels))),\n",
    "    }, labels, Xs\n",
    "\n",
    "results = []\n",
    "artifacts = {}\n",
    "\n",
    "for b in betas:\n",
    "    Zb, Yb = beta_Z[b]\n",
    "    X_fused = np.concatenate([Zb, txt_vecs[:len(Zb)], genre_oh[:len(Zb)]], axis=1)\n",
    "    row, lab, Xs = run_kmeans_eval(X_fused, Yb, K, name=f'HARD_FUSED_BetaVAE(beta={b})+KMeans')\n",
    "    results.append(row)\n",
    "    artifacts[row['method']] = (lab, Xs, Yb)\n",
    "\n",
    "row, lab, Xs = run_kmeans_eval(Z_ae, Y_ae, K, name='BASE_AE(audio_z)+KMeans')\n",
    "results.append(row)\n",
    "artifacts[row['method']] = (lab, Xs, Y_ae)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=['nmi','ari','purity','silhouette'], ascending=False)\n",
    "results_df.to_csv(os.path.join(OUT_DIR, 'hard_all_results_metrics.csv'), index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(results_df['method'], results_df['nmi'])\n",
    "plt.xticks(rotation=25, ha='right')\n",
    "plt.title('NMI (higher is better)')\n",
    "save_fig('hard_nmi_bar.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(results_df['method'], results_df['purity'])\n",
    "plt.xticks(rotation=25, ha='right')\n",
    "plt.title('Purity (higher is better)')\n",
    "save_fig('hard_purity_bar.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "best_method = results_df.iloc[0]['method']\n",
    "labels_pred, Xs_best, y_true_best = artifacts[best_method]\n",
    "\n",
    "n = min(1500, Xs_best.shape[0])\n",
    "idx = np.random.permutation(Xs_best.shape[0])[:n]\n",
    "Z = TSNE(n_components=2, random_state=SEED, init='pca', learning_rate='auto', perplexity=30).fit_transform(Xs_best[idx])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y_true_best[idx], s=10)\n",
    "plt.title(f't-SNE | {best_method} | colored by GENRE')\n",
    "save_fig('hard_tsne_by_genre.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Z[:,0], Z[:,1], c=lang[:len(idx)], s=10)\n",
    "plt.title(f't-SNE | {best_method} | colored by LANGUAGE')\n",
    "save_fig('hard_tsne_by_language.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
